{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Aim of this exercise is to classify the websites based on their content. Here, we have got a dataset which contains list of websites, their kind and text extract from them.\n",
    "\n",
    "Here, we would be using `Natural Language Processing(NLP)` techniques to develop out dataset and then create a Naive Bayes model based on it to classify websites.\n",
    "\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. Data exploring and understanding: This step involves preliminary level data understanding and exploring.\n",
    "\n",
    "2. Data Cleaning: This step includes cleaning the existing data. We check the data for any missing values and treat them as per the requirements. We also need to look for constant value columns as that is not going to add any additional value to out analysis. Sometimes columns with very high proportion of any particular value also doesn't add any values. Hence, getting rid of them helps with further analysis.\n",
    "\n",
    "3. Data Preparation: This step is mainly useful for feeding in the data into the model. It involves steps like creating dummy variables, scaling etc. depending upon the data type.  Here, we would be using `NLP` techniques to develop our dataset which can be fed to model.\n",
    "\n",
    "4. Data Visualization: This step involves visualizing our dataset and check for relationship amongst independent variables. We can also reduce some feature columns here but it should not be aggressive.\n",
    "\n",
    "5. Train-test split: This step involves splitting the dataset into train and test parts.\n",
    "\n",
    "6. Model Development-validation and evaluation: This steps involves training the model and validate it. It involves evaluating the model using relevant metrics.\n",
    "\n",
    "7. Conclusion/Recommendation: It involves drawing conclusions and recommendations to business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:06.113712Z",
     "iopub.status.busy": "2022-11-06T08:02:06.113355Z",
     "iopub.status.idle": "2022-11-06T08:02:07.943386Z",
     "shell.execute_reply": "2022-11-06T08:02:07.941678Z",
     "shell.execute_reply.started": "2022-11-06T08:02:06.113636Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:07.945987Z",
     "iopub.status.busy": "2022-11-06T08:02:07.945719Z",
     "iopub.status.idle": "2022-11-06T08:02:07.960385Z",
     "shell.execute_reply": "2022-11-06T08:02:07.958772Z",
     "shell.execute_reply.started": "2022-11-06T08:02:07.945963Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames: \n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:07.962779Z",
     "iopub.status.busy": "2022-11-06T08:02:07.962498Z",
     "iopub.status.idle": "2022-11-06T08:02:08.188825Z",
     "shell.execute_reply": "2022-11-06T08:02:08.188074Z",
     "shell.execute_reply.started": "2022-11-06T08:02:07.962754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>website_url</th>\n",
       "      <th>cleaned_website_text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.booking.com/index.html?aid=1743217</td>\n",
       "      <td>official site good hotel accommodation big sav...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://travelsites.com/expedia/</td>\n",
       "      <td>expedia hotel book sites like use vacation wor...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://travelsites.com/tripadvisor/</td>\n",
       "      <td>tripadvisor hotel book sites like previously d...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.momondo.in/?ispredir=true</td>\n",
       "      <td>cheap flights search compare flights momondo f...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...</td>\n",
       "      <td>bot create free account create free account si...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        website_url  \\\n",
       "0           0     https://www.booking.com/index.html?aid=1743217   \n",
       "1           1                   https://travelsites.com/expedia/   \n",
       "2           2               https://travelsites.com/tripadvisor/   \n",
       "3           3              https://www.momondo.in/?ispredir=true   \n",
       "4           4  https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...   \n",
       "\n",
       "                                cleaned_website_text Category  \n",
       "0  official site good hotel accommodation big sav...   Travel  \n",
       "1  expedia hotel book sites like use vacation wor...   Travel  \n",
       "2  tripadvisor hotel book sites like previously d...   Travel  \n",
       "3  cheap flights search compare flights momondo f...   Travel  \n",
       "4  bot create free account create free account si...   Travel  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('website_classification.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is having 4 columns including target variable `Category`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploring and understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:08.190118Z",
     "iopub.status.busy": "2022-11-06T08:02:08.189901Z",
     "iopub.status.idle": "2022-11-06T08:02:08.195692Z",
     "shell.execute_reply": "2022-11-06T08:02:08.194586Z",
     "shell.execute_reply.started": "2022-11-06T08:02:08.190096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1408, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is having 1408 entries.\n",
    "Here we have first column which indicates serial hence we would be dropping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:08.197252Z",
     "iopub.status.busy": "2022-11-06T08:02:08.197013Z",
     "iopub.status.idle": "2022-11-06T08:02:08.217478Z",
     "shell.execute_reply": "2022-11-06T08:02:08.215854Z",
     "shell.execute_reply.started": "2022-11-06T08:02:08.197228Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_url</th>\n",
       "      <th>cleaned_website_text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.booking.com/index.html?aid=1743217</td>\n",
       "      <td>official site good hotel accommodation big sav...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://travelsites.com/expedia/</td>\n",
       "      <td>expedia hotel book sites like use vacation wor...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://travelsites.com/tripadvisor/</td>\n",
       "      <td>tripadvisor hotel book sites like previously d...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.momondo.in/?ispredir=true</td>\n",
       "      <td>cheap flights search compare flights momondo f...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...</td>\n",
       "      <td>bot create free account create free account si...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         website_url  \\\n",
       "0     https://www.booking.com/index.html?aid=1743217   \n",
       "1                   https://travelsites.com/expedia/   \n",
       "2               https://travelsites.com/tripadvisor/   \n",
       "3              https://www.momondo.in/?ispredir=true   \n",
       "4  https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...   \n",
       "\n",
       "                                cleaned_website_text Category  \n",
       "0  official site good hotel accommodation big sav...   Travel  \n",
       "1  expedia hotel book sites like use vacation wor...   Travel  \n",
       "2  tripadvisor hotel book sites like previously d...   Travel  \n",
       "3  cheap flights search compare flights momondo f...   Travel  \n",
       "4  bot create free account create free account si...   Travel  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:08.219493Z",
     "iopub.status.busy": "2022-11-06T08:02:08.218993Z",
     "iopub.status.idle": "2022-11-06T08:02:08.242589Z",
     "shell.execute_reply": "2022-11-06T08:02:08.241885Z",
     "shell.execute_reply.started": "2022-11-06T08:02:08.219461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1408 entries, 0 to 1407\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   website_url           1408 non-null   object\n",
      " 1   cleaned_website_text  1408 non-null   object\n",
      " 2   Category              1408 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 33.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is not having any null entry hence we do not need to treat it for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:08.243887Z",
     "iopub.status.busy": "2022-11-06T08:02:08.243569Z",
     "iopub.status.idle": "2022-11-06T08:02:08.262133Z",
     "shell.execute_reply": "2022-11-06T08:02:08.260748Z",
     "shell.execute_reply.started": "2022-11-06T08:02:08.243858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# website_url\n",
    "\n",
    "data.website_url.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that `website_url` column is having 1384 unique entries. That means we have some duplicate entries in our dataset. We would be dropping those entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:08.264420Z",
     "iopub.status.busy": "2022-11-06T08:02:08.264089Z",
     "iopub.status.idle": "2022-11-06T08:02:08.278126Z",
     "shell.execute_reply": "2022-11-06T08:02:08.276875Z",
     "shell.execute_reply.started": "2022-11-06T08:02:08.264392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1384, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping duplicate entries\n",
    "\n",
    "data = data.drop_duplicates(subset='website_url').reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:08.279903Z",
     "iopub.status.busy": "2022-11-06T08:02:08.279678Z",
     "iopub.status.idle": "2022-11-06T08:02:08.290483Z",
     "shell.execute_reply": "2022-11-06T08:02:08.289156Z",
     "shell.execute_reply.started": "2022-11-06T08:02:08.279880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Education                          114\n",
       "Business/Corporate                 108\n",
       "Travel                             107\n",
       "Streaming Services                 104\n",
       "E-Commerce                         101\n",
       "Sports                             100\n",
       "Games                               98\n",
       "News                                94\n",
       "Photography                         92\n",
       "Food                                92\n",
       "Computers and Technology            91\n",
       "Health and Fitness                  88\n",
       "Law and Government                  83\n",
       "Social Networking and Messaging     80\n",
       "Forums                              16\n",
       "Adult                               16\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Category\n",
    "\n",
    "data.Category.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Category` column is having 16 unique entries, which means this is a multi-class classification problem. Hence, we would be using `MultinomialNB` algorithm to solve it. MultinomialNB is efficient algorithm and can handle multi-class problems easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:08.292454Z",
     "iopub.status.busy": "2022-11-06T08:02:08.292150Z",
     "iopub.status.idle": "2022-11-06T08:02:08.305924Z",
     "shell.execute_reply": "2022-11-06T08:02:08.304502Z",
     "shell.execute_reply.started": "2022-11-06T08:02:08.292430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       official site good hotel accommodation big sav...\n",
       "1       expedia hotel book sites like use vacation wor...\n",
       "2       tripadvisor hotel book sites like previously d...\n",
       "3       cheap flights search compare flights momondo f...\n",
       "4       bot create free account create free account si...\n",
       "                              ...                        \n",
       "1379    old nude women porn mature granny sex horny ol...\n",
       "1380    bdsm cams bdsm chat bondage cams free bdsm vid...\n",
       "1381    porno dvd online european porn dvd cheap adult...\n",
       "1382    anal dream house anal dream house anal dream h...\n",
       "1383    world sex news daily sex news adult news eroti...\n",
       "Name: cleaned_website_text, Length: 1384, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaned_website_text\n",
    "\n",
    "data.cleaned_website_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cleaned_website_text` column contains the content text from the website. We would be using this column to develop features for out model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature matrix development method:\n",
    "\n",
    "We would extract words from `cleaned_website_text` columns and create an array of those words. Out of that array we would filter out english language stop_words which generally doesn't give any meaningful insight about the document. Out of the remaining words we would chose top `n` most recurring words and consider them as feature. \n",
    "\n",
    "Next part is to assign feature value to each of the data entry. For assigning feature value, we would use `TF-IFD` method.\n",
    "\n",
    "`TF-IDF:` TF-IDF is a method of comparing how relevant a word is to any particular document (here website) from a given bunch of documents. \n",
    "\n",
    "$TF-IDF = TF(t,d)*IDF(t,D)$\n",
    "\n",
    "where, \n",
    "\n",
    "TF(t,d) = (count of t in d)/(number of words in d)\n",
    "\n",
    "$IDF(t,D) = log(D/(df + 1))$\n",
    "\n",
    "$t$--> term of interest\n",
    "\n",
    "$d$ --> document of interest\n",
    "\n",
    "$D$ --> Set of documents\n",
    "\n",
    "$df$ --> occurrence of t in documents\n",
    "\n",
    "To summarize, TF-IDF score of any feature for any particular document would be high if that feature term occurs rarely in other documents and/or percentage occurrence for that term in the given document is higher.\n",
    "\n",
    "\n",
    "Additional Reading Source: \n",
    "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before developing TF-IDF based array we need to filter out words with `Parts Of Speech (POS)` other than `NOUN`. The logic behind it is that `NOUN` gives better representation of website content as compared to other `POS`.\n",
    "\n",
    "Here we would be using `NLTK` library to tokenize the corpus from each website and assign `POS` tag to each word. Then we would filter in only `NOUN` tagged words and reverse convert them into corpus. The filtered words corpus would be meaningless for us but can enhance the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:02:08.308152Z",
     "iopub.status.busy": "2022-11-06T08:02:08.307687Z",
     "iopub.status.idle": "2022-11-06T08:03:13.989258Z",
     "shell.execute_reply": "2022-11-06T08:03:13.987930Z",
     "shell.execute_reply.started": "2022-11-06T08:02:08.308125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtering noun words from corpus\n",
    "\n",
    "filtered_words = []\n",
    "for i in range(data.shape[0]):\n",
    "    tagged = nltk.pos_tag(word_tokenize(data.cleaned_website_text[i]), tagset='universal')\n",
    "    filtered_words.append(' '.join([word_tag[0] for word_tag in tagged if word_tag[1]=='NOUN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:13.990784Z",
     "iopub.status.busy": "2022-11-06T08:03:13.990537Z",
     "iopub.status.idle": "2022-11-06T08:03:14.003012Z",
     "shell.execute_reply": "2022-11-06T08:03:14.001917Z",
     "shell.execute_reply.started": "2022-11-06T08:03:13.990760Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding filtered_words to our dataframe\n",
    "\n",
    "data['filtered_words'] = filtered_words\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's develop TF-IDF based array using `filtered_words` column from out dataset. Here we have selected some hyperparameters as per following.\n",
    "\n",
    "- min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. We would use 5 as min frequency.\n",
    "\n",
    "- stop_words: We would remove English stopwords if at all they are present in the filtered words. Stopwords adds no meaning to the model.\n",
    "\n",
    "- max_features: Maximum features to be included the the data. We are going to use 1500 features. \n",
    "\n",
    "Note: Naive Bayes algorithm can easily handle larger number of features as compared to data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:14.004772Z",
     "iopub.status.busy": "2022-11-06T08:03:14.004336Z",
     "iopub.status.idle": "2022-11-06T08:03:14.607436Z",
     "shell.execute_reply": "2022-11-06T08:03:14.605989Z",
     "shell.execute_reply.started": "2022-11-06T08:03:14.004738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorized form of tfidf \n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                        min_df=5,\n",
    "                        stop_words = 'english',\n",
    "                        max_features = 1500)\n",
    "feat = tfidf.fit_transform(data.filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:14.608987Z",
     "iopub.status.busy": "2022-11-06T08:03:14.608756Z",
     "iopub.status.idle": "2022-11-06T08:03:14.629511Z",
     "shell.execute_reply": "2022-11-06T08:03:14.628303Z",
     "shell.execute_reply.started": "2022-11-06T08:03:14.608963Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature/term names\n",
    "\n",
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:14.631191Z",
     "iopub.status.busy": "2022-11-06T08:03:14.630892Z",
     "iopub.status.idle": "2022-11-06T08:03:14.669262Z",
     "shell.execute_reply": "2022-11-06T08:03:14.668580Z",
     "shell.execute_reply.started": "2022-11-06T08:03:14.631159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting tfidfvector to dataframe\n",
    "\n",
    "X = pd.DataFrame(feat.toarray(), columns = tfidf.get_feature_names())\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:14.670682Z",
     "iopub.status.busy": "2022-11-06T08:03:14.670376Z",
     "iopub.status.idle": "2022-11-06T08:03:17.204996Z",
     "shell.execute_reply": "2022-11-06T08:03:17.204077Z",
     "shell.execute_reply.started": "2022-11-06T08:03:14.670650Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking range for values in X\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:17.206159Z",
     "iopub.status.busy": "2022-11-06T08:03:17.205862Z",
     "iopub.status.idle": "2022-11-06T08:03:17.211762Z",
     "shell.execute_reply": "2022-11-06T08:03:17.211032Z",
     "shell.execute_reply.started": "2022-11-06T08:03:17.206137Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoding labes to derive y\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(data.Category)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:17.212907Z",
     "iopub.status.busy": "2022-11-06T08:03:17.212681Z",
     "iopub.status.idle": "2022-11-06T08:03:17.258694Z",
     "shell.execute_reply": "2022-11-06T08:03:17.257490Z",
     "shell.execute_reply.started": "2022-11-06T08:03:17.212877Z"
    }
   },
   "outputs": [],
   "source": [
    "df_f = pd.concat([X, data.Category], axis=1)\n",
    "df_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:17.259928Z",
     "iopub.status.busy": "2022-11-06T08:03:17.259689Z",
     "iopub.status.idle": "2022-11-06T08:03:18.305569Z",
     "shell.execute_reply": "2022-11-06T08:03:18.304453Z",
     "shell.execute_reply.started": "2022-11-06T08:03:17.259900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,15))\n",
    "for i, col in enumerate(df_f.columns[0:5]):\n",
    "    plt.subplot(int(df_f.columns[0:5].shape[0]/2)+1, 2, i+1)\n",
    "    df_f.groupby('Category').mean()[col].plot(kind='barh', title=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations,\n",
    "\n",
    "- Term `accessories` is more related to `E-Commerce`. \n",
    "- Term `accessibility` is more related to `Law and Government`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:18.306938Z",
     "iopub.status.busy": "2022-11-06T08:03:18.306705Z",
     "iopub.status.idle": "2022-11-06T08:03:18.318836Z",
     "shell.execute_reply": "2022-11-06T08:03:18.317445Z",
     "shell.execute_reply.started": "2022-11-06T08:03:18.306914Z"
    }
   },
   "outputs": [],
   "source": [
    "# train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                     train_size = 0.7, \n",
    "                                     test_size = 0.3, \n",
    "                                     random_state = 100,\n",
    "                                     stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Development-validation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:18.320581Z",
     "iopub.status.busy": "2022-11-06T08:03:18.320305Z",
     "iopub.status.idle": "2022-11-06T08:03:18.348353Z",
     "shell.execute_reply": "2022-11-06T08:03:18.347647Z",
     "shell.execute_reply.started": "2022-11-06T08:03:18.320553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model development \n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:18.349785Z",
     "iopub.status.busy": "2022-11-06T08:03:18.349339Z",
     "iopub.status.idle": "2022-11-06T08:03:18.393205Z",
     "shell.execute_reply": "2022-11-06T08:03:18.392482Z",
     "shell.execute_reply.started": "2022-11-06T08:03:18.349755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making prediction and evaluation\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy check\n",
    "\n",
    "print('Train accuracy:', round(accuracy_score(y_train, y_train_pred),2))\n",
    "print('Test accuracy:', round(accuracy_score(y_test, y_test_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:18.396056Z",
     "iopub.status.busy": "2022-11-06T08:03:18.395658Z",
     "iopub.status.idle": "2022-11-06T08:03:19.272664Z",
     "shell.execute_reply": "2022-11-06T08:03:19.271712Z",
     "shell.execute_reply.started": "2022-11-06T08:03:18.396028Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,8))\n",
    "sns.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot=True,\n",
    "           cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T08:03:19.274238Z",
     "iopub.status.busy": "2022-11-06T08:03:19.274010Z",
     "iopub.status.idle": "2022-11-06T08:03:20.257145Z",
     "shell.execute_reply": "2022-11-06T08:03:20.255584Z",
     "shell.execute_reply.started": "2022-11-06T08:03:19.274216Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,8))\n",
    "sns.heatmap(confusion_matrix(y_test, y_test_pred),\n",
    "           annot=True,\n",
    "           cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix indicates that,\n",
    "\n",
    "- Model is not able to estimate category 0 and 6 correctly both while training and testing. The reason can be their lesser frequency.\n",
    "- Model is clearly not overfitting which is a good sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion/Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP can be used for text processing and feature development to be used in ML model building.\n",
    "- Multinomial Naive Bayes algorithm can be used to perform multi-class classification with large number of classes.\n",
    "- Naive Bayes in general can handle large number of features as compared to data size quite easily.\n",
    "- For future work, we can increase number of data entries for classes with less frequency and hence improve the model further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
